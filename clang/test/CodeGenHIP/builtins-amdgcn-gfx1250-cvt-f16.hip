// NOTE: Assertions have been autogenerated by utils/update_cc_test_checks.py UTC_ARGS: --version 6
// REQUIRES: amdgpu-registered-target
// RUN: %clang_cc1 -triple amdgcn-amd-amdhsa -target-cpu gfx1250 -emit-llvm -fcuda-is-device -o - %s | FileCheck %s

#define __device__ __attribute__((device))

typedef _Float16 v2h __attribute__((ext_vector_type(2)));
typedef _Float16 v8h __attribute__((ext_vector_type(8)));
typedef _Float16 v16h __attribute__((ext_vector_type(16)));
typedef unsigned int v2ui __attribute__((ext_vector_type(2)));
typedef unsigned int v3ui __attribute__((ext_vector_type(3)));

// cvt_sr_pk_f16_f32: _ExtVector<2, _Float16>(float, float, int)
// CHECK-LABEL: define dso_local void @_Z22test_cvt_sr_pk_f16_f32PDv2_DF16_ffi(
// CHECK-SAME: ptr noundef [[OUT:%.*]], float noundef [[A:%.*]], float noundef [[B:%.*]], i32 noundef [[SR:%.*]]) #[[ATTR0:[0-9]+]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[OUT_ADDR:%.*]] = alloca ptr, align 8, addrspace(5)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca float, align 4, addrspace(5)
// CHECK-NEXT:    [[B_ADDR:%.*]] = alloca float, align 4, addrspace(5)
// CHECK-NEXT:    [[SR_ADDR:%.*]] = alloca i32, align 4, addrspace(5)
// CHECK-NEXT:    [[OUT_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[OUT_ADDR]] to ptr
// CHECK-NEXT:    [[A_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[A_ADDR]] to ptr
// CHECK-NEXT:    [[B_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[B_ADDR]] to ptr
// CHECK-NEXT:    [[SR_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[SR_ADDR]] to ptr
// CHECK-NEXT:    store ptr [[OUT]], ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store float [[A]], ptr [[A_ADDR_ASCAST]], align 4
// CHECK-NEXT:    store float [[B]], ptr [[B_ADDR_ASCAST]], align 4
// CHECK-NEXT:    store i32 [[SR]], ptr [[SR_ADDR_ASCAST]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load float, ptr [[A_ADDR_ASCAST]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load float, ptr [[B_ADDR_ASCAST]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[SR_ADDR_ASCAST]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = call contract <2 x half> @llvm.amdgcn.cvt.sr.pk.f16.f32(float [[TMP0]], float [[TMP1]], i32 [[TMP2]])
// CHECK-NEXT:    [[TMP4:%.*]] = load ptr, ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store <2 x half> [[TMP3]], ptr [[TMP4]], align 4
// CHECK-NEXT:    ret void
//
__device__ void test_cvt_sr_pk_f16_f32(v2h *out, float a, float b, int sr) {
  *out = __builtin_amdgcn_cvt_sr_pk_f16_f32(a, b, sr);
}

// cvt_f16_fp8: _Float16(int, _Constant int)
// CHECK-LABEL: define dso_local void @_Z16test_cvt_f16_fp8PDF16_i(
// CHECK-SAME: ptr noundef [[OUT:%.*]], i32 noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[OUT_ADDR:%.*]] = alloca ptr, align 8, addrspace(5)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4, addrspace(5)
// CHECK-NEXT:    [[OUT_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[OUT_ADDR]] to ptr
// CHECK-NEXT:    [[A_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[A_ADDR]] to ptr
// CHECK-NEXT:    store ptr [[OUT]], ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store i32 [[A]], ptr [[A_ADDR_ASCAST]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR_ASCAST]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call contract half @llvm.amdgcn.cvt.f16.fp8(i32 [[TMP0]], i32 0)
// CHECK-NEXT:    [[TMP2:%.*]] = load ptr, ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds half, ptr [[TMP2]], i64 0
// CHECK-NEXT:    store half [[TMP1]], ptr [[ARRAYIDX]], align 2
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, ptr [[A_ADDR_ASCAST]], align 4
// CHECK-NEXT:    [[TMP4:%.*]] = call contract half @llvm.amdgcn.cvt.f16.fp8(i32 [[TMP3]], i32 1)
// CHECK-NEXT:    [[TMP5:%.*]] = load ptr, ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    [[ARRAYIDX1:%.*]] = getelementptr inbounds half, ptr [[TMP5]], i64 1
// CHECK-NEXT:    store half [[TMP4]], ptr [[ARRAYIDX1]], align 2
// CHECK-NEXT:    ret void
//
__device__ void test_cvt_f16_fp8(_Float16 *out, int a) {
  out[0] = __builtin_amdgcn_cvt_f16_fp8(a, 0);
  out[1] = __builtin_amdgcn_cvt_f16_fp8(a, 1);
}

// cvt_f16_bf8: _Float16(int, _Constant int)
// CHECK-LABEL: define dso_local void @_Z16test_cvt_f16_bf8PDF16_i(
// CHECK-SAME: ptr noundef [[OUT:%.*]], i32 noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[OUT_ADDR:%.*]] = alloca ptr, align 8, addrspace(5)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i32, align 4, addrspace(5)
// CHECK-NEXT:    [[OUT_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[OUT_ADDR]] to ptr
// CHECK-NEXT:    [[A_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[A_ADDR]] to ptr
// CHECK-NEXT:    store ptr [[OUT]], ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store i32 [[A]], ptr [[A_ADDR_ASCAST]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[A_ADDR_ASCAST]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call contract half @llvm.amdgcn.cvt.f16.bf8(i32 [[TMP0]], i32 0)
// CHECK-NEXT:    [[TMP2:%.*]] = load ptr, ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    [[ARRAYIDX:%.*]] = getelementptr inbounds half, ptr [[TMP2]], i64 0
// CHECK-NEXT:    store half [[TMP1]], ptr [[ARRAYIDX]], align 2
// CHECK-NEXT:    [[TMP3:%.*]] = load i32, ptr [[A_ADDR_ASCAST]], align 4
// CHECK-NEXT:    [[TMP4:%.*]] = call contract half @llvm.amdgcn.cvt.f16.bf8(i32 [[TMP3]], i32 1)
// CHECK-NEXT:    [[TMP5:%.*]] = load ptr, ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    [[ARRAYIDX1:%.*]] = getelementptr inbounds half, ptr [[TMP5]], i64 1
// CHECK-NEXT:    store half [[TMP4]], ptr [[ARRAYIDX1]], align 2
// CHECK-NEXT:    ret void
//
__device__ void test_cvt_f16_bf8(_Float16 *out, int a) {
  out[0] = __builtin_amdgcn_cvt_f16_bf8(a, 0);
  out[1] = __builtin_amdgcn_cvt_f16_bf8(a, 1);
}

// cvt_pk_f16_fp8: _ExtVector<2, _Float16>(short)
// CHECK-LABEL: define dso_local void @_Z19test_cvt_pk_f16_fp8PDv2_DF16_s(
// CHECK-SAME: ptr noundef [[OUT:%.*]], i16 noundef signext [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[OUT_ADDR:%.*]] = alloca ptr, align 8, addrspace(5)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i16, align 2, addrspace(5)
// CHECK-NEXT:    [[OUT_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[OUT_ADDR]] to ptr
// CHECK-NEXT:    [[A_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[A_ADDR]] to ptr
// CHECK-NEXT:    store ptr [[OUT]], ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store i16 [[A]], ptr [[A_ADDR_ASCAST]], align 2
// CHECK-NEXT:    [[TMP0:%.*]] = load i16, ptr [[A_ADDR_ASCAST]], align 2
// CHECK-NEXT:    [[TMP1:%.*]] = call contract <2 x half> @llvm.amdgcn.cvt.pk.f16.fp8(i16 [[TMP0]])
// CHECK-NEXT:    [[TMP2:%.*]] = load ptr, ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store <2 x half> [[TMP1]], ptr [[TMP2]], align 4
// CHECK-NEXT:    ret void
//
__device__ void test_cvt_pk_f16_fp8(v2h *out, short a) {
  *out = __builtin_amdgcn_cvt_pk_f16_fp8(a);
}

// cvt_pk_f16_bf8: _ExtVector<2, _Float16>(short)
// CHECK-LABEL: define dso_local void @_Z19test_cvt_pk_f16_bf8PDv2_DF16_s(
// CHECK-SAME: ptr noundef [[OUT:%.*]], i16 noundef signext [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[OUT_ADDR:%.*]] = alloca ptr, align 8, addrspace(5)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca i16, align 2, addrspace(5)
// CHECK-NEXT:    [[OUT_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[OUT_ADDR]] to ptr
// CHECK-NEXT:    [[A_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[A_ADDR]] to ptr
// CHECK-NEXT:    store ptr [[OUT]], ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store i16 [[A]], ptr [[A_ADDR_ASCAST]], align 2
// CHECK-NEXT:    [[TMP0:%.*]] = load i16, ptr [[A_ADDR_ASCAST]], align 2
// CHECK-NEXT:    [[TMP1:%.*]] = call contract <2 x half> @llvm.amdgcn.cvt.pk.f16.bf8(i16 [[TMP0]])
// CHECK-NEXT:    [[TMP2:%.*]] = load ptr, ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store <2 x half> [[TMP1]], ptr [[TMP2]], align 4
// CHECK-NEXT:    ret void
//
__device__ void test_cvt_pk_f16_bf8(v2h *out, short a) {
  *out = __builtin_amdgcn_cvt_pk_f16_bf8(a);
}

// cvt_pk_fp8_f16: short(_ExtVector<2, _Float16>)
// CHECK-LABEL: define dso_local void @_Z19test_cvt_pk_fp8_f16PsDv2_DF16_(
// CHECK-SAME: ptr noundef [[OUT:%.*]], <2 x half> noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[OUT_ADDR:%.*]] = alloca ptr, align 8, addrspace(5)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <2 x half>, align 4, addrspace(5)
// CHECK-NEXT:    [[OUT_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[OUT_ADDR]] to ptr
// CHECK-NEXT:    [[A_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[A_ADDR]] to ptr
// CHECK-NEXT:    store ptr [[OUT]], ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store <2 x half> [[A]], ptr [[A_ADDR_ASCAST]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load <2 x half>, ptr [[A_ADDR_ASCAST]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i16 @llvm.amdgcn.cvt.pk.fp8.f16(<2 x half> [[TMP0]])
// CHECK-NEXT:    [[TMP2:%.*]] = load ptr, ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store i16 [[TMP1]], ptr [[TMP2]], align 2
// CHECK-NEXT:    ret void
//
__device__ void test_cvt_pk_fp8_f16(short *out, v2h a) {
  *out = __builtin_amdgcn_cvt_pk_fp8_f16(a);
}

// cvt_pk_bf8_f16: short(_ExtVector<2, _Float16>)
// CHECK-LABEL: define dso_local void @_Z19test_cvt_pk_bf8_f16PsDv2_DF16_(
// CHECK-SAME: ptr noundef [[OUT:%.*]], <2 x half> noundef [[A:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[OUT_ADDR:%.*]] = alloca ptr, align 8, addrspace(5)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca <2 x half>, align 4, addrspace(5)
// CHECK-NEXT:    [[OUT_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[OUT_ADDR]] to ptr
// CHECK-NEXT:    [[A_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[A_ADDR]] to ptr
// CHECK-NEXT:    store ptr [[OUT]], ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store <2 x half> [[A]], ptr [[A_ADDR_ASCAST]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load <2 x half>, ptr [[A_ADDR_ASCAST]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = call i16 @llvm.amdgcn.cvt.pk.bf8.f16(<2 x half> [[TMP0]])
// CHECK-NEXT:    [[TMP2:%.*]] = load ptr, ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store i16 [[TMP1]], ptr [[TMP2]], align 2
// CHECK-NEXT:    ret void
//
__device__ void test_cvt_pk_bf8_f16(short *out, v2h a) {
  *out = __builtin_amdgcn_cvt_pk_bf8_f16(a);
}

// cvt_sr_fp8_f16: int(_Float16, int, unsigned int, _Constant int)
// CHECK-LABEL: define dso_local void @_Z19test_cvt_sr_fp8_f16PiDF16_ij(
// CHECK-SAME: ptr noundef [[OUT:%.*]], half noundef [[A:%.*]], i32 noundef [[SR:%.*]], i32 noundef [[OLD:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[OUT_ADDR:%.*]] = alloca ptr, align 8, addrspace(5)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca half, align 2, addrspace(5)
// CHECK-NEXT:    [[SR_ADDR:%.*]] = alloca i32, align 4, addrspace(5)
// CHECK-NEXT:    [[OLD_ADDR:%.*]] = alloca i32, align 4, addrspace(5)
// CHECK-NEXT:    [[OUT_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[OUT_ADDR]] to ptr
// CHECK-NEXT:    [[A_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[A_ADDR]] to ptr
// CHECK-NEXT:    [[SR_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[SR_ADDR]] to ptr
// CHECK-NEXT:    [[OLD_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[OLD_ADDR]] to ptr
// CHECK-NEXT:    store ptr [[OUT]], ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store half [[A]], ptr [[A_ADDR_ASCAST]], align 2
// CHECK-NEXT:    store i32 [[SR]], ptr [[SR_ADDR_ASCAST]], align 4
// CHECK-NEXT:    store i32 [[OLD]], ptr [[OLD_ADDR_ASCAST]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load half, ptr [[A_ADDR_ASCAST]], align 2
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[SR_ADDR_ASCAST]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[OLD_ADDR_ASCAST]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = call i32 @llvm.amdgcn.cvt.sr.fp8.f16(half [[TMP0]], i32 [[TMP1]], i32 [[TMP2]], i32 0)
// CHECK-NEXT:    [[TMP4:%.*]] = load ptr, ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store i32 [[TMP3]], ptr [[TMP4]], align 4
// CHECK-NEXT:    ret void
//
__device__ void test_cvt_sr_fp8_f16(int *out, _Float16 a, int sr, unsigned int old) {
  *out = __builtin_amdgcn_cvt_sr_fp8_f16(a, sr, old, 0);
}

// cvt_sr_bf8_f16: int(_Float16, int, unsigned int, _Constant int)
// CHECK-LABEL: define dso_local void @_Z19test_cvt_sr_bf8_f16PiDF16_ij(
// CHECK-SAME: ptr noundef [[OUT:%.*]], half noundef [[A:%.*]], i32 noundef [[SR:%.*]], i32 noundef [[OLD:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[OUT_ADDR:%.*]] = alloca ptr, align 8, addrspace(5)
// CHECK-NEXT:    [[A_ADDR:%.*]] = alloca half, align 2, addrspace(5)
// CHECK-NEXT:    [[SR_ADDR:%.*]] = alloca i32, align 4, addrspace(5)
// CHECK-NEXT:    [[OLD_ADDR:%.*]] = alloca i32, align 4, addrspace(5)
// CHECK-NEXT:    [[OUT_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[OUT_ADDR]] to ptr
// CHECK-NEXT:    [[A_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[A_ADDR]] to ptr
// CHECK-NEXT:    [[SR_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[SR_ADDR]] to ptr
// CHECK-NEXT:    [[OLD_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[OLD_ADDR]] to ptr
// CHECK-NEXT:    store ptr [[OUT]], ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store half [[A]], ptr [[A_ADDR_ASCAST]], align 2
// CHECK-NEXT:    store i32 [[SR]], ptr [[SR_ADDR_ASCAST]], align 4
// CHECK-NEXT:    store i32 [[OLD]], ptr [[OLD_ADDR_ASCAST]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load half, ptr [[A_ADDR_ASCAST]], align 2
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[SR_ADDR_ASCAST]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = load i32, ptr [[OLD_ADDR_ASCAST]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = call i32 @llvm.amdgcn.cvt.sr.bf8.f16(half [[TMP0]], i32 [[TMP1]], i32 [[TMP2]], i32 0)
// CHECK-NEXT:    [[TMP4:%.*]] = load ptr, ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store i32 [[TMP3]], ptr [[TMP4]], align 4
// CHECK-NEXT:    ret void
//
__device__ void test_cvt_sr_bf8_f16(int *out, _Float16 a, int sr, unsigned int old) {
  *out = __builtin_amdgcn_cvt_sr_bf8_f16(a, sr, old, 0);
}

// cvt_scale_pk8_f16_fp8: _ExtVector<8, _Float16>(_ExtVector<2, unsigned int>, unsigned int, _Constant unsigned int)
// CHECK-LABEL: define dso_local void @_Z26test_cvt_scale_pk8_f16_fp8PDv8_DF16_Dv2_jj(
// CHECK-SAME: ptr noundef [[OUT:%.*]], <2 x i32> noundef [[SRC:%.*]], i32 noundef [[SCALE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[OUT_ADDR:%.*]] = alloca ptr, align 8, addrspace(5)
// CHECK-NEXT:    [[SRC_ADDR:%.*]] = alloca <2 x i32>, align 8, addrspace(5)
// CHECK-NEXT:    [[SCALE_ADDR:%.*]] = alloca i32, align 4, addrspace(5)
// CHECK-NEXT:    [[OUT_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[OUT_ADDR]] to ptr
// CHECK-NEXT:    [[SRC_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[SRC_ADDR]] to ptr
// CHECK-NEXT:    [[SCALE_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[SCALE_ADDR]] to ptr
// CHECK-NEXT:    store ptr [[OUT]], ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store <2 x i32> [[SRC]], ptr [[SRC_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store i32 [[SCALE]], ptr [[SCALE_ADDR_ASCAST]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load <2 x i32>, ptr [[SRC_ADDR_ASCAST]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[SCALE_ADDR_ASCAST]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call contract <8 x half> @llvm.amdgcn.cvt.scale.pk8.f16.fp8(<2 x i32> [[TMP0]], i32 [[TMP1]], i32 0)
// CHECK-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store <8 x half> [[TMP2]], ptr [[TMP3]], align 16
// CHECK-NEXT:    ret void
//
__device__ void test_cvt_scale_pk8_f16_fp8(v8h *out, v2ui src, unsigned int scale) {
  *out = __builtin_amdgcn_cvt_scale_pk8_f16_fp8(src, scale, 0);
}

// cvt_scale_pk8_f16_bf8: _ExtVector<8, _Float16>(_ExtVector<2, unsigned int>, unsigned int, _Constant unsigned int)
// CHECK-LABEL: define dso_local void @_Z26test_cvt_scale_pk8_f16_bf8PDv8_DF16_Dv2_jj(
// CHECK-SAME: ptr noundef [[OUT:%.*]], <2 x i32> noundef [[SRC:%.*]], i32 noundef [[SCALE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[OUT_ADDR:%.*]] = alloca ptr, align 8, addrspace(5)
// CHECK-NEXT:    [[SRC_ADDR:%.*]] = alloca <2 x i32>, align 8, addrspace(5)
// CHECK-NEXT:    [[SCALE_ADDR:%.*]] = alloca i32, align 4, addrspace(5)
// CHECK-NEXT:    [[OUT_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[OUT_ADDR]] to ptr
// CHECK-NEXT:    [[SRC_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[SRC_ADDR]] to ptr
// CHECK-NEXT:    [[SCALE_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[SCALE_ADDR]] to ptr
// CHECK-NEXT:    store ptr [[OUT]], ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store <2 x i32> [[SRC]], ptr [[SRC_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store i32 [[SCALE]], ptr [[SCALE_ADDR_ASCAST]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load <2 x i32>, ptr [[SRC_ADDR_ASCAST]], align 8
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[SCALE_ADDR_ASCAST]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call contract <8 x half> @llvm.amdgcn.cvt.scale.pk8.f16.bf8(<2 x i32> [[TMP0]], i32 [[TMP1]], i32 0)
// CHECK-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store <8 x half> [[TMP2]], ptr [[TMP3]], align 16
// CHECK-NEXT:    ret void
//
__device__ void test_cvt_scale_pk8_f16_bf8(v8h *out, v2ui src, unsigned int scale) {
  *out = __builtin_amdgcn_cvt_scale_pk8_f16_bf8(src, scale, 0);
}

// cvt_scale_pk8_f16_fp4: _ExtVector<8, _Float16>(unsigned int, unsigned int, _Constant unsigned int)
// CHECK-LABEL: define dso_local void @_Z26test_cvt_scale_pk8_f16_fp4PDv8_DF16_jj(
// CHECK-SAME: ptr noundef [[OUT:%.*]], i32 noundef [[SRC:%.*]], i32 noundef [[SCALE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[OUT_ADDR:%.*]] = alloca ptr, align 8, addrspace(5)
// CHECK-NEXT:    [[SRC_ADDR:%.*]] = alloca i32, align 4, addrspace(5)
// CHECK-NEXT:    [[SCALE_ADDR:%.*]] = alloca i32, align 4, addrspace(5)
// CHECK-NEXT:    [[OUT_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[OUT_ADDR]] to ptr
// CHECK-NEXT:    [[SRC_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[SRC_ADDR]] to ptr
// CHECK-NEXT:    [[SCALE_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[SCALE_ADDR]] to ptr
// CHECK-NEXT:    store ptr [[OUT]], ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store i32 [[SRC]], ptr [[SRC_ADDR_ASCAST]], align 4
// CHECK-NEXT:    store i32 [[SCALE]], ptr [[SCALE_ADDR_ASCAST]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load i32, ptr [[SRC_ADDR_ASCAST]], align 4
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[SCALE_ADDR_ASCAST]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call contract <8 x half> @llvm.amdgcn.cvt.scale.pk8.f16.fp4(i32 [[TMP0]], i32 [[TMP1]], i32 0)
// CHECK-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store <8 x half> [[TMP2]], ptr [[TMP3]], align 16
// CHECK-NEXT:    ret void
//
__device__ void test_cvt_scale_pk8_f16_fp4(v8h *out, unsigned int src, unsigned int scale) {
  *out = __builtin_amdgcn_cvt_scale_pk8_f16_fp4(src, scale, 0);
}

// cvt_scale_pk16_f16_fp6: _ExtVector<16, _Float16>(_ExtVector<3, unsigned int>, unsigned int, _Constant unsigned int)
// CHECK-LABEL: define dso_local void @_Z27test_cvt_scale_pk16_f16_fp6PDv16_DF16_Dv3_jj(
// CHECK-SAME: ptr noundef [[OUT:%.*]], <3 x i32> noundef [[SRC:%.*]], i32 noundef [[SCALE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[OUT_ADDR:%.*]] = alloca ptr, align 8, addrspace(5)
// CHECK-NEXT:    [[SRC_ADDR:%.*]] = alloca <3 x i32>, align 16, addrspace(5)
// CHECK-NEXT:    [[SCALE_ADDR:%.*]] = alloca i32, align 4, addrspace(5)
// CHECK-NEXT:    [[OUT_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[OUT_ADDR]] to ptr
// CHECK-NEXT:    [[SRC_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[SRC_ADDR]] to ptr
// CHECK-NEXT:    [[SCALE_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[SCALE_ADDR]] to ptr
// CHECK-NEXT:    store ptr [[OUT]], ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store <3 x i32> [[SRC]], ptr [[SRC_ADDR_ASCAST]], align 16
// CHECK-NEXT:    store i32 [[SCALE]], ptr [[SCALE_ADDR_ASCAST]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load <3 x i32>, ptr [[SRC_ADDR_ASCAST]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[SCALE_ADDR_ASCAST]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call contract <16 x half> @llvm.amdgcn.cvt.scale.pk16.f16.fp6(<3 x i32> [[TMP0]], i32 [[TMP1]], i32 0)
// CHECK-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store <16 x half> [[TMP2]], ptr [[TMP3]], align 32
// CHECK-NEXT:    ret void
//
__device__ void test_cvt_scale_pk16_f16_fp6(v16h *out, v3ui src, unsigned int scale) {
  *out = __builtin_amdgcn_cvt_scale_pk16_f16_fp6(src, scale, 0);
}

// cvt_scale_pk16_f16_bf6: _ExtVector<16, _Float16>(_ExtVector<3, unsigned int>, unsigned int, _Constant unsigned int)
// CHECK-LABEL: define dso_local void @_Z27test_cvt_scale_pk16_f16_bf6PDv16_DF16_Dv3_jj(
// CHECK-SAME: ptr noundef [[OUT:%.*]], <3 x i32> noundef [[SRC:%.*]], i32 noundef [[SCALE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[OUT_ADDR:%.*]] = alloca ptr, align 8, addrspace(5)
// CHECK-NEXT:    [[SRC_ADDR:%.*]] = alloca <3 x i32>, align 16, addrspace(5)
// CHECK-NEXT:    [[SCALE_ADDR:%.*]] = alloca i32, align 4, addrspace(5)
// CHECK-NEXT:    [[OUT_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[OUT_ADDR]] to ptr
// CHECK-NEXT:    [[SRC_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[SRC_ADDR]] to ptr
// CHECK-NEXT:    [[SCALE_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[SCALE_ADDR]] to ptr
// CHECK-NEXT:    store ptr [[OUT]], ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store <3 x i32> [[SRC]], ptr [[SRC_ADDR_ASCAST]], align 16
// CHECK-NEXT:    store i32 [[SCALE]], ptr [[SCALE_ADDR_ASCAST]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load <3 x i32>, ptr [[SRC_ADDR_ASCAST]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[SCALE_ADDR_ASCAST]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call contract <16 x half> @llvm.amdgcn.cvt.scale.pk16.f16.bf6(<3 x i32> [[TMP0]], i32 [[TMP1]], i32 0)
// CHECK-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store <16 x half> [[TMP2]], ptr [[TMP3]], align 32
// CHECK-NEXT:    ret void
//
__device__ void test_cvt_scale_pk16_f16_bf6(v16h *out, v3ui src, unsigned int scale) {
  *out = __builtin_amdgcn_cvt_scale_pk16_f16_bf6(src, scale, 0);
}

// cvt_scalef32_pk8_fp8_f16: _ExtVector<2, unsigned int>(_ExtVector<8, _Float16>, float)
// CHECK-LABEL: define dso_local void @_Z29test_cvt_scalef32_pk8_fp8_f16PDv2_jDv8_DF16_f(
// CHECK-SAME: ptr noundef [[OUT:%.*]], <8 x half> noundef [[SRC:%.*]], float noundef [[SCALE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[OUT_ADDR:%.*]] = alloca ptr, align 8, addrspace(5)
// CHECK-NEXT:    [[SRC_ADDR:%.*]] = alloca <8 x half>, align 16, addrspace(5)
// CHECK-NEXT:    [[SCALE_ADDR:%.*]] = alloca float, align 4, addrspace(5)
// CHECK-NEXT:    [[OUT_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[OUT_ADDR]] to ptr
// CHECK-NEXT:    [[SRC_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[SRC_ADDR]] to ptr
// CHECK-NEXT:    [[SCALE_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[SCALE_ADDR]] to ptr
// CHECK-NEXT:    store ptr [[OUT]], ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store <8 x half> [[SRC]], ptr [[SRC_ADDR_ASCAST]], align 16
// CHECK-NEXT:    store float [[SCALE]], ptr [[SCALE_ADDR_ASCAST]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[SRC_ADDR_ASCAST]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load float, ptr [[SCALE_ADDR_ASCAST]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call <2 x i32> @llvm.amdgcn.cvt.scalef32.pk8.fp8.f16(<8 x half> [[TMP0]], float [[TMP1]])
// CHECK-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store <2 x i32> [[TMP2]], ptr [[TMP3]], align 8
// CHECK-NEXT:    ret void
//
__device__ void test_cvt_scalef32_pk8_fp8_f16(v2ui *out, v8h src, float scale) {
  *out = __builtin_amdgcn_cvt_scalef32_pk8_fp8_f16(src, scale);
}

// cvt_scalef32_pk8_bf8_f16: _ExtVector<2, unsigned int>(_ExtVector<8, _Float16>, float)
// CHECK-LABEL: define dso_local void @_Z29test_cvt_scalef32_pk8_bf8_f16PDv2_jDv8_DF16_f(
// CHECK-SAME: ptr noundef [[OUT:%.*]], <8 x half> noundef [[SRC:%.*]], float noundef [[SCALE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[OUT_ADDR:%.*]] = alloca ptr, align 8, addrspace(5)
// CHECK-NEXT:    [[SRC_ADDR:%.*]] = alloca <8 x half>, align 16, addrspace(5)
// CHECK-NEXT:    [[SCALE_ADDR:%.*]] = alloca float, align 4, addrspace(5)
// CHECK-NEXT:    [[OUT_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[OUT_ADDR]] to ptr
// CHECK-NEXT:    [[SRC_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[SRC_ADDR]] to ptr
// CHECK-NEXT:    [[SCALE_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[SCALE_ADDR]] to ptr
// CHECK-NEXT:    store ptr [[OUT]], ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store <8 x half> [[SRC]], ptr [[SRC_ADDR_ASCAST]], align 16
// CHECK-NEXT:    store float [[SCALE]], ptr [[SCALE_ADDR_ASCAST]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[SRC_ADDR_ASCAST]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load float, ptr [[SCALE_ADDR_ASCAST]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call <2 x i32> @llvm.amdgcn.cvt.scalef32.pk8.bf8.f16(<8 x half> [[TMP0]], float [[TMP1]])
// CHECK-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store <2 x i32> [[TMP2]], ptr [[TMP3]], align 8
// CHECK-NEXT:    ret void
//
__device__ void test_cvt_scalef32_pk8_bf8_f16(v2ui *out, v8h src, float scale) {
  *out = __builtin_amdgcn_cvt_scalef32_pk8_bf8_f16(src, scale);
}

// cvt_scalef32_pk8_fp4_f16: unsigned int(_ExtVector<8, _Float16>, float)
// CHECK-LABEL: define dso_local void @_Z29test_cvt_scalef32_pk8_fp4_f16PjDv8_DF16_f(
// CHECK-SAME: ptr noundef [[OUT:%.*]], <8 x half> noundef [[SRC:%.*]], float noundef [[SCALE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[OUT_ADDR:%.*]] = alloca ptr, align 8, addrspace(5)
// CHECK-NEXT:    [[SRC_ADDR:%.*]] = alloca <8 x half>, align 16, addrspace(5)
// CHECK-NEXT:    [[SCALE_ADDR:%.*]] = alloca float, align 4, addrspace(5)
// CHECK-NEXT:    [[OUT_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[OUT_ADDR]] to ptr
// CHECK-NEXT:    [[SRC_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[SRC_ADDR]] to ptr
// CHECK-NEXT:    [[SCALE_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[SCALE_ADDR]] to ptr
// CHECK-NEXT:    store ptr [[OUT]], ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store <8 x half> [[SRC]], ptr [[SRC_ADDR_ASCAST]], align 16
// CHECK-NEXT:    store float [[SCALE]], ptr [[SCALE_ADDR_ASCAST]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[SRC_ADDR_ASCAST]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load float, ptr [[SCALE_ADDR_ASCAST]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call i32 @llvm.amdgcn.cvt.scalef32.pk8.fp4.f16(<8 x half> [[TMP0]], float [[TMP1]])
// CHECK-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store i32 [[TMP2]], ptr [[TMP3]], align 4
// CHECK-NEXT:    ret void
//
__device__ void test_cvt_scalef32_pk8_fp4_f16(unsigned int *out, v8h src, float scale) {
  *out = __builtin_amdgcn_cvt_scalef32_pk8_fp4_f16(src, scale);
}

// cvt_scalef32_pk16_fp6_f16: _ExtVector<3, unsigned int>(_ExtVector<16, _Float16>, float)
// CHECK-LABEL: define dso_local void @_Z30test_cvt_scalef32_pk16_fp6_f16PDv3_jDv16_DF16_f(
// CHECK-SAME: ptr noundef [[OUT:%.*]], <16 x half> noundef [[SRC:%.*]], float noundef [[SCALE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[OUT_ADDR:%.*]] = alloca ptr, align 8, addrspace(5)
// CHECK-NEXT:    [[SRC_ADDR:%.*]] = alloca <16 x half>, align 32, addrspace(5)
// CHECK-NEXT:    [[SCALE_ADDR:%.*]] = alloca float, align 4, addrspace(5)
// CHECK-NEXT:    [[OUT_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[OUT_ADDR]] to ptr
// CHECK-NEXT:    [[SRC_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[SRC_ADDR]] to ptr
// CHECK-NEXT:    [[SCALE_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[SCALE_ADDR]] to ptr
// CHECK-NEXT:    store ptr [[OUT]], ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store <16 x half> [[SRC]], ptr [[SRC_ADDR_ASCAST]], align 32
// CHECK-NEXT:    store float [[SCALE]], ptr [[SCALE_ADDR_ASCAST]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load <16 x half>, ptr [[SRC_ADDR_ASCAST]], align 32
// CHECK-NEXT:    [[TMP1:%.*]] = load float, ptr [[SCALE_ADDR_ASCAST]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call <3 x i32> @llvm.amdgcn.cvt.scalef32.pk16.fp6.f16(<16 x half> [[TMP0]], float [[TMP1]])
// CHECK-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store <3 x i32> [[TMP2]], ptr [[TMP3]], align 16
// CHECK-NEXT:    ret void
//
__device__ void test_cvt_scalef32_pk16_fp6_f16(v3ui *out, v16h src, float scale) {
  *out = __builtin_amdgcn_cvt_scalef32_pk16_fp6_f16(src, scale);
}

// cvt_scalef32_pk16_bf6_f16: _ExtVector<3, unsigned int>(_ExtVector<16, _Float16>, float)
// CHECK-LABEL: define dso_local void @_Z30test_cvt_scalef32_pk16_bf6_f16PDv3_jDv16_DF16_f(
// CHECK-SAME: ptr noundef [[OUT:%.*]], <16 x half> noundef [[SRC:%.*]], float noundef [[SCALE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[OUT_ADDR:%.*]] = alloca ptr, align 8, addrspace(5)
// CHECK-NEXT:    [[SRC_ADDR:%.*]] = alloca <16 x half>, align 32, addrspace(5)
// CHECK-NEXT:    [[SCALE_ADDR:%.*]] = alloca float, align 4, addrspace(5)
// CHECK-NEXT:    [[OUT_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[OUT_ADDR]] to ptr
// CHECK-NEXT:    [[SRC_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[SRC_ADDR]] to ptr
// CHECK-NEXT:    [[SCALE_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[SCALE_ADDR]] to ptr
// CHECK-NEXT:    store ptr [[OUT]], ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store <16 x half> [[SRC]], ptr [[SRC_ADDR_ASCAST]], align 32
// CHECK-NEXT:    store float [[SCALE]], ptr [[SCALE_ADDR_ASCAST]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load <16 x half>, ptr [[SRC_ADDR_ASCAST]], align 32
// CHECK-NEXT:    [[TMP1:%.*]] = load float, ptr [[SCALE_ADDR_ASCAST]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = call <3 x i32> @llvm.amdgcn.cvt.scalef32.pk16.bf6.f16(<16 x half> [[TMP0]], float [[TMP1]])
// CHECK-NEXT:    [[TMP3:%.*]] = load ptr, ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store <3 x i32> [[TMP2]], ptr [[TMP3]], align 16
// CHECK-NEXT:    ret void
//
__device__ void test_cvt_scalef32_pk16_bf6_f16(v3ui *out, v16h src, float scale) {
  *out = __builtin_amdgcn_cvt_scalef32_pk16_bf6_f16(src, scale);
}

// cvt_scalef32_sr_pk8_fp8_f16: _ExtVector<2, unsigned int>(_ExtVector<8, _Float16>, unsigned int, float)
// CHECK-LABEL: define dso_local void @_Z32test_cvt_scalef32_sr_pk8_fp8_f16PDv2_jDv8_DF16_jf(
// CHECK-SAME: ptr noundef [[OUT:%.*]], <8 x half> noundef [[SRC:%.*]], i32 noundef [[SR:%.*]], float noundef [[SCALE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[OUT_ADDR:%.*]] = alloca ptr, align 8, addrspace(5)
// CHECK-NEXT:    [[SRC_ADDR:%.*]] = alloca <8 x half>, align 16, addrspace(5)
// CHECK-NEXT:    [[SR_ADDR:%.*]] = alloca i32, align 4, addrspace(5)
// CHECK-NEXT:    [[SCALE_ADDR:%.*]] = alloca float, align 4, addrspace(5)
// CHECK-NEXT:    [[OUT_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[OUT_ADDR]] to ptr
// CHECK-NEXT:    [[SRC_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[SRC_ADDR]] to ptr
// CHECK-NEXT:    [[SR_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[SR_ADDR]] to ptr
// CHECK-NEXT:    [[SCALE_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[SCALE_ADDR]] to ptr
// CHECK-NEXT:    store ptr [[OUT]], ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store <8 x half> [[SRC]], ptr [[SRC_ADDR_ASCAST]], align 16
// CHECK-NEXT:    store i32 [[SR]], ptr [[SR_ADDR_ASCAST]], align 4
// CHECK-NEXT:    store float [[SCALE]], ptr [[SCALE_ADDR_ASCAST]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[SRC_ADDR_ASCAST]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[SR_ADDR_ASCAST]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = load float, ptr [[SCALE_ADDR_ASCAST]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = call <2 x i32> @llvm.amdgcn.cvt.scalef32.sr.pk8.fp8.f16(<8 x half> [[TMP0]], i32 [[TMP1]], float [[TMP2]])
// CHECK-NEXT:    [[TMP4:%.*]] = load ptr, ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store <2 x i32> [[TMP3]], ptr [[TMP4]], align 8
// CHECK-NEXT:    ret void
//
__device__ void test_cvt_scalef32_sr_pk8_fp8_f16(v2ui *out, v8h src, unsigned int sr, float scale) {
  *out = __builtin_amdgcn_cvt_scalef32_sr_pk8_fp8_f16(src, sr, scale);
}

// cvt_scalef32_sr_pk8_bf8_f16: _ExtVector<2, unsigned int>(_ExtVector<8, _Float16>, unsigned int, float)
// CHECK-LABEL: define dso_local void @_Z32test_cvt_scalef32_sr_pk8_bf8_f16PDv2_jDv8_DF16_jf(
// CHECK-SAME: ptr noundef [[OUT:%.*]], <8 x half> noundef [[SRC:%.*]], i32 noundef [[SR:%.*]], float noundef [[SCALE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[OUT_ADDR:%.*]] = alloca ptr, align 8, addrspace(5)
// CHECK-NEXT:    [[SRC_ADDR:%.*]] = alloca <8 x half>, align 16, addrspace(5)
// CHECK-NEXT:    [[SR_ADDR:%.*]] = alloca i32, align 4, addrspace(5)
// CHECK-NEXT:    [[SCALE_ADDR:%.*]] = alloca float, align 4, addrspace(5)
// CHECK-NEXT:    [[OUT_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[OUT_ADDR]] to ptr
// CHECK-NEXT:    [[SRC_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[SRC_ADDR]] to ptr
// CHECK-NEXT:    [[SR_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[SR_ADDR]] to ptr
// CHECK-NEXT:    [[SCALE_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[SCALE_ADDR]] to ptr
// CHECK-NEXT:    store ptr [[OUT]], ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store <8 x half> [[SRC]], ptr [[SRC_ADDR_ASCAST]], align 16
// CHECK-NEXT:    store i32 [[SR]], ptr [[SR_ADDR_ASCAST]], align 4
// CHECK-NEXT:    store float [[SCALE]], ptr [[SCALE_ADDR_ASCAST]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[SRC_ADDR_ASCAST]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[SR_ADDR_ASCAST]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = load float, ptr [[SCALE_ADDR_ASCAST]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = call <2 x i32> @llvm.amdgcn.cvt.scalef32.sr.pk8.bf8.f16(<8 x half> [[TMP0]], i32 [[TMP1]], float [[TMP2]])
// CHECK-NEXT:    [[TMP4:%.*]] = load ptr, ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store <2 x i32> [[TMP3]], ptr [[TMP4]], align 8
// CHECK-NEXT:    ret void
//
__device__ void test_cvt_scalef32_sr_pk8_bf8_f16(v2ui *out, v8h src, unsigned int sr, float scale) {
  *out = __builtin_amdgcn_cvt_scalef32_sr_pk8_bf8_f16(src, sr, scale);
}

// cvt_scalef32_sr_pk8_fp4_f16: unsigned int(_ExtVector<8, _Float16>, unsigned int, float)
// CHECK-LABEL: define dso_local void @_Z32test_cvt_scalef32_sr_pk8_fp4_f16PjDv8_DF16_jf(
// CHECK-SAME: ptr noundef [[OUT:%.*]], <8 x half> noundef [[SRC:%.*]], i32 noundef [[SR:%.*]], float noundef [[SCALE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[OUT_ADDR:%.*]] = alloca ptr, align 8, addrspace(5)
// CHECK-NEXT:    [[SRC_ADDR:%.*]] = alloca <8 x half>, align 16, addrspace(5)
// CHECK-NEXT:    [[SR_ADDR:%.*]] = alloca i32, align 4, addrspace(5)
// CHECK-NEXT:    [[SCALE_ADDR:%.*]] = alloca float, align 4, addrspace(5)
// CHECK-NEXT:    [[OUT_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[OUT_ADDR]] to ptr
// CHECK-NEXT:    [[SRC_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[SRC_ADDR]] to ptr
// CHECK-NEXT:    [[SR_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[SR_ADDR]] to ptr
// CHECK-NEXT:    [[SCALE_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[SCALE_ADDR]] to ptr
// CHECK-NEXT:    store ptr [[OUT]], ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store <8 x half> [[SRC]], ptr [[SRC_ADDR_ASCAST]], align 16
// CHECK-NEXT:    store i32 [[SR]], ptr [[SR_ADDR_ASCAST]], align 4
// CHECK-NEXT:    store float [[SCALE]], ptr [[SCALE_ADDR_ASCAST]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load <8 x half>, ptr [[SRC_ADDR_ASCAST]], align 16
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[SR_ADDR_ASCAST]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = load float, ptr [[SCALE_ADDR_ASCAST]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = call i32 @llvm.amdgcn.cvt.scalef32.sr.pk8.fp4.f16(<8 x half> [[TMP0]], i32 [[TMP1]], float [[TMP2]])
// CHECK-NEXT:    [[TMP4:%.*]] = load ptr, ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store i32 [[TMP3]], ptr [[TMP4]], align 4
// CHECK-NEXT:    ret void
//
__device__ void test_cvt_scalef32_sr_pk8_fp4_f16(unsigned int *out, v8h src, unsigned int sr, float scale) {
  *out = __builtin_amdgcn_cvt_scalef32_sr_pk8_fp4_f16(src, sr, scale);
}

// cvt_scalef32_sr_pk16_bf6_f16: _ExtVector<3, unsigned int>(_ExtVector<16, _Float16>, unsigned int, float)
// CHECK-LABEL: define dso_local void @_Z33test_cvt_scalef32_sr_pk16_bf6_f16PDv3_jDv16_DF16_jf(
// CHECK-SAME: ptr noundef [[OUT:%.*]], <16 x half> noundef [[SRC:%.*]], i32 noundef [[SR:%.*]], float noundef [[SCALE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[OUT_ADDR:%.*]] = alloca ptr, align 8, addrspace(5)
// CHECK-NEXT:    [[SRC_ADDR:%.*]] = alloca <16 x half>, align 32, addrspace(5)
// CHECK-NEXT:    [[SR_ADDR:%.*]] = alloca i32, align 4, addrspace(5)
// CHECK-NEXT:    [[SCALE_ADDR:%.*]] = alloca float, align 4, addrspace(5)
// CHECK-NEXT:    [[OUT_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[OUT_ADDR]] to ptr
// CHECK-NEXT:    [[SRC_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[SRC_ADDR]] to ptr
// CHECK-NEXT:    [[SR_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[SR_ADDR]] to ptr
// CHECK-NEXT:    [[SCALE_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[SCALE_ADDR]] to ptr
// CHECK-NEXT:    store ptr [[OUT]], ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store <16 x half> [[SRC]], ptr [[SRC_ADDR_ASCAST]], align 32
// CHECK-NEXT:    store i32 [[SR]], ptr [[SR_ADDR_ASCAST]], align 4
// CHECK-NEXT:    store float [[SCALE]], ptr [[SCALE_ADDR_ASCAST]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load <16 x half>, ptr [[SRC_ADDR_ASCAST]], align 32
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[SR_ADDR_ASCAST]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = load float, ptr [[SCALE_ADDR_ASCAST]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = call <3 x i32> @llvm.amdgcn.cvt.scalef32.sr.pk16.bf6.f16(<16 x half> [[TMP0]], i32 [[TMP1]], float [[TMP2]])
// CHECK-NEXT:    [[TMP4:%.*]] = load ptr, ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store <3 x i32> [[TMP3]], ptr [[TMP4]], align 16
// CHECK-NEXT:    ret void
//
__device__ void test_cvt_scalef32_sr_pk16_bf6_f16(v3ui *out, v16h src, unsigned int sr, float scale) {
  *out = __builtin_amdgcn_cvt_scalef32_sr_pk16_bf6_f16(src, sr, scale);
}

// cvt_scalef32_sr_pk16_fp6_f16: _ExtVector<3, unsigned int>(_ExtVector<16, _Float16>, unsigned int, float)
// CHECK-LABEL: define dso_local void @_Z33test_cvt_scalef32_sr_pk16_fp6_f16PDv3_jDv16_DF16_jf(
// CHECK-SAME: ptr noundef [[OUT:%.*]], <16 x half> noundef [[SRC:%.*]], i32 noundef [[SR:%.*]], float noundef [[SCALE:%.*]]) #[[ATTR0]] {
// CHECK-NEXT:  [[ENTRY:.*:]]
// CHECK-NEXT:    [[OUT_ADDR:%.*]] = alloca ptr, align 8, addrspace(5)
// CHECK-NEXT:    [[SRC_ADDR:%.*]] = alloca <16 x half>, align 32, addrspace(5)
// CHECK-NEXT:    [[SR_ADDR:%.*]] = alloca i32, align 4, addrspace(5)
// CHECK-NEXT:    [[SCALE_ADDR:%.*]] = alloca float, align 4, addrspace(5)
// CHECK-NEXT:    [[OUT_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[OUT_ADDR]] to ptr
// CHECK-NEXT:    [[SRC_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[SRC_ADDR]] to ptr
// CHECK-NEXT:    [[SR_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[SR_ADDR]] to ptr
// CHECK-NEXT:    [[SCALE_ADDR_ASCAST:%.*]] = addrspacecast ptr addrspace(5) [[SCALE_ADDR]] to ptr
// CHECK-NEXT:    store ptr [[OUT]], ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store <16 x half> [[SRC]], ptr [[SRC_ADDR_ASCAST]], align 32
// CHECK-NEXT:    store i32 [[SR]], ptr [[SR_ADDR_ASCAST]], align 4
// CHECK-NEXT:    store float [[SCALE]], ptr [[SCALE_ADDR_ASCAST]], align 4
// CHECK-NEXT:    [[TMP0:%.*]] = load <16 x half>, ptr [[SRC_ADDR_ASCAST]], align 32
// CHECK-NEXT:    [[TMP1:%.*]] = load i32, ptr [[SR_ADDR_ASCAST]], align 4
// CHECK-NEXT:    [[TMP2:%.*]] = load float, ptr [[SCALE_ADDR_ASCAST]], align 4
// CHECK-NEXT:    [[TMP3:%.*]] = call <3 x i32> @llvm.amdgcn.cvt.scalef32.sr.pk16.fp6.f16(<16 x half> [[TMP0]], i32 [[TMP1]], float [[TMP2]])
// CHECK-NEXT:    [[TMP4:%.*]] = load ptr, ptr [[OUT_ADDR_ASCAST]], align 8
// CHECK-NEXT:    store <3 x i32> [[TMP3]], ptr [[TMP4]], align 16
// CHECK-NEXT:    ret void
//
__device__ void test_cvt_scalef32_sr_pk16_fp6_f16(v3ui *out, v16h src, unsigned int sr, float scale) {
  *out = __builtin_amdgcn_cvt_scalef32_sr_pk16_fp6_f16(src, sr, scale);
}
